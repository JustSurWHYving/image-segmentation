{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Import Packages"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2022-11-27T20:19:06.283917Z","iopub.status.busy":"2022-11-27T20:19:06.283141Z","iopub.status.idle":"2022-11-27T20:19:11.935808Z","shell.execute_reply":"2022-11-27T20:19:11.934726Z","shell.execute_reply.started":"2022-11-27T20:19:06.283818Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from tqdm import tqdm \n","from PIL import Image\n","import os\n","import warnings\n","\n","from keras.src.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, Dropout, concatenate\n","from keras.src.callbacks import ModelCheckpoint\n","\n","sns.set_style('darkgrid')\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","metadata":{},"source":["# Import labeling key from GitHub\n","\n","The following code below is found in the following GitHub repository: https://github.molgen.mpg.de/mohomran/cityscapes/blob/master/scripts/helpers/labels.py#L55\n","\n","This repository helper function was created by one of the team members of the CityScapes dataset"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-11-27T20:19:11.943001Z","iopub.status.busy":"2022-11-27T20:19:11.942383Z","iopub.status.idle":"2022-11-27T20:19:11.96779Z","shell.execute_reply":"2022-11-27T20:19:11.96684Z","shell.execute_reply.started":"2022-11-27T20:19:11.94296Z"},"trusted":true},"outputs":[],"source":["from collections import namedtuple\n","\n","#--------------------------------------------------------------------------------\n","# Definitions\n","#--------------------------------------------------------------------------------\n","\n","# a label and all meta information\n","Label = namedtuple( 'Label' , [\n","\n","    'name'        , # The identifier of this label, e.g. 'car', 'person', ... .\n","                    # We use them to uniquely name a class\n","\n","    'id'          , # An integer ID that is associated with this label.\n","                    # The IDs are used to represent the label in ground truth images\n","                    # An ID of -1 means that this label does not have an ID and thus\n","                    # is ignored when creating ground truth images (e.g. license plate).\n","\n","    'trainId'     , # An integer ID that overwrites the ID above, when creating ground truth\n","                    # images for training.\n","                    # For training, multiple labels might have the same ID. Then, these labels\n","                    # are mapped to the same class in the ground truth images. For the inverse\n","                    # mapping, we use the label that is defined first in the list below.\n","                    # For example, mapping all void-type classes to the same ID in training,\n","                    # might make sense for some approaches.\n","\n","    'category'    , # The name of the category that this label belongs to\n","\n","    'categoryId'  , # The ID of this category. Used to create ground truth images\n","                    # on category level.\n","\n","    'hasInstances', # Whether this label distinguishes between single instances or not\n","\n","    'ignoreInEval', # Whether pixels having this class as ground truth label are ignored\n","                    # during evaluations or not\n","\n","    'color'       , # The color of this label\n","    ] )\n","\n","\n","#--------------------------------------------------------------------------------\n","# A list of all labels\n","#--------------------------------------------------------------------------------\n","\n","# Please adapt the train IDs as appropriate for you approach.\n","# Note that you might want to ignore labels with ID 255 during training.\n","# Make sure to provide your results using the original IDs and not the training IDs.\n","# Note that many IDs are ignored in evaluation and thus you never need to predict these!\n","\n","labels = [\n","    #       name                     id    trainId   category            catId     hasInstances   ignoreInEval   color\n","    Label(  'unlabeled'            ,  0 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n","    Label(  'ego vehicle'          ,  1 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n","    Label(  'rectification border' ,  2 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n","    Label(  'out of roi'           ,  3 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n","    Label(  'static'               ,  4 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n","    Label(  'dynamic'              ,  5 ,      255 , 'void'            , 0       , False        , True         , (111, 74,  0) ),\n","    Label(  'ground'               ,  6 ,      255 , 'void'            , 0       , False        , True         , ( 81,  0, 81) ),\n","    Label(  'road'                 ,  7 ,        0 , 'ground'          , 1       , False        , False        , (128, 64,128) ),\n","    Label(  'sidewalk'             ,  8 ,        1 , 'ground'          , 1       , False        , False        , (244, 35,232) ),\n","    Label(  'parking'              ,  9 ,      255 , 'ground'          , 1       , False        , True         , (250,170,160) ),\n","    Label(  'rail track'           , 10 ,      255 , 'ground'          , 1       , False        , True         , (230,150,140) ),\n","    Label(  'building'             , 11 ,        2 , 'construction'    , 2       , False        , False        , ( 70, 70, 70) ),\n","    Label(  'wall'                 , 12 ,        3 , 'construction'    , 2       , False        , False        , (102,102,156) ),\n","    Label(  'fence'                , 13 ,        4 , 'construction'    , 2       , False        , False        , (190,153,153) ),\n","    Label(  'guard rail'           , 14 ,      255 , 'construction'    , 2       , False        , True         , (180,165,180) ),\n","    Label(  'bridge'               , 15 ,      255 , 'construction'    , 2       , False        , True         , (150,100,100) ),\n","    Label(  'tunnel'               , 16 ,      255 , 'construction'    , 2       , False        , True         , (150,120, 90) ),\n","    Label(  'pole'                 , 17 ,        5 , 'object'          , 3       , False        , False        , (153,153,153) ),\n","    Label(  'polegroup'            , 18 ,      255 , 'object'          , 3       , False        , True         , (153,153,153) ),\n","    Label(  'traffic light'        , 19 ,        6 , 'object'          , 3       , False        , False        , (250,170, 30) ),\n","    Label(  'traffic sign'         , 20 ,        7 , 'object'          , 3       , False        , False        , (220,220,  0) ),\n","    Label(  'vegetation'           , 21 ,        8 , 'nature'          , 4       , False        , False        , (107,142, 35) ),\n","    Label(  'terrain'              , 22 ,        9 , 'nature'          , 4       , False        , False        , (152,251,152) ),\n","    Label(  'sky'                  , 23 ,       10 , 'sky'             , 5       , False        , False        , ( 70,130,180) ),\n","    Label(  'person'               , 24 ,       11 , 'human'           , 6       , True         , False        , (220, 20, 60) ),\n","    Label(  'rider'                , 25 ,       12 , 'human'           , 6       , True         , False        , (255,  0,  0) ),\n","    Label(  'car'                  , 26 ,       13 , 'vehicle'         , 7       , True         , False        , (  0,  0,142) ),\n","    Label(  'truck'                , 27 ,       14 , 'vehicle'         , 7       , True         , False        , (  0,  0, 70) ),\n","    Label(  'bus'                  , 28 ,       15 , 'vehicle'         , 7       , True         , False        , (  0, 60,100) ),\n","    Label(  'caravan'              , 29 ,      255 , 'vehicle'         , 7       , True         , True         , (  0,  0, 90) ),\n","    Label(  'trailer'              , 30 ,      255 , 'vehicle'         , 7       , True         , True         , (  0,  0,110) ),\n","    Label(  'train'                , 31 ,       16 , 'vehicle'         , 7       , True         , False        , (  0, 80,100) ),\n","    Label(  'motorcycle'           , 32 ,       17 , 'vehicle'         , 7       , True         , False        , (  0,  0,230) ),\n","    Label(  'bicycle'              , 33 ,       18 , 'vehicle'         , 7       , True         , False        , (119, 11, 32) ),\n","    Label(  'license plate'        , 34 ,       19 , 'vehicle'         , 7       , False        , True         , (  0,  0,142) ),\n","]"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-11-27T20:19:11.969634Z","iopub.status.busy":"2022-11-27T20:19:11.969177Z","iopub.status.idle":"2022-11-27T20:19:11.978891Z","shell.execute_reply":"2022-11-27T20:19:11.977911Z","shell.execute_reply.started":"2022-11-27T20:19:11.969592Z"},"trusted":true},"outputs":[],"source":["N_FILTERS = 64\n","KERNEL_SIZE = 3\n","N_CLASSES = len(labels)\n","IMAGE_SIZE = [128, 128]\n","IMAGE_SHAPE = IMAGE_SIZE + [3,]\n","\n","EPOCHS = 40\n","BATCH_SIZE = 16\n","MODEL_CHECKPOINT_FILEPATH = './cityscapes-unet.ckpt'\n","\n","id2color = { label.id : np.asarray(label.color) for label in labels }"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-11-27T20:19:11.983711Z","iopub.status.busy":"2022-11-27T20:19:11.983281Z","iopub.status.idle":"2022-11-27T20:19:11.993613Z","shell.execute_reply":"2022-11-27T20:19:11.992531Z","shell.execute_reply.started":"2022-11-27T20:19:11.983684Z"},"trusted":true},"outputs":[],"source":["#--------------------------------------------------------------------------------\n","#  Load images in, crop for the image and mask, resize, and then encode mask\n","#--------------------------------------------------------------------------------\n","\n","def image_mask_split(filename, image_size):\n","    image_mask = Image.open(filename)\n","    \n","    image, mask = image_mask.crop([0, 0, 256, 256]), image_mask.crop([256, 0, 512, 256])\n","    image = image.resize(image_size)\n","    mask = mask.resize(image_size)\n","\n","    image = np.array(image) / 255 # crop image section and reformat as normalized np array\n","    mask = np.array(mask) # crop mask section and reformat as np array\n","    \n","    return image, mask\n","\n","#--------------------------------------------------------------------------------\n","# Remap mask half of image into sparse matrix using closest color value\n","#--------------------------------------------------------------------------------\n","\n","def find_closest_labels_vectorized(mask, mapping): # 'mapping' is a RGB color tuple to categorical number dictionary\n","    \n","    closest_distance = np.full([mask.shape[0], mask.shape[1]], 10000) \n","    closest_category = np.full([mask.shape[0], mask.shape[1]], None)   \n","\n","    for id, color in mapping.items(): # iterate over every color mapping\n","        dist = np.sqrt(np.linalg.norm(mask - color.reshape([1,1,-1]), axis=-1))\n","        is_closer = closest_distance > dist\n","        closest_distance = np.where(is_closer, dist, closest_distance)\n","        closest_category = np.where(is_closer, id, closest_category)\n","    \n","    return closest_category"]},{"cell_type":"markdown","metadata":{},"source":["# Load Datasets\n","The images will be loaded in using the functions defined above. The mask half of the image will need to be encoded into usable categorical values.\n","\n","One issue that arose when first attempting to encode is that the image is not cleanly segmented into section.  The boundaries between categories have intermediary values.  This is most likely an artifact from anti-aliasing when the images were being resized into 256 x 512 pixels. Thus the encoding performs a loop through every categorical variables color and finds the one that is has the closest vector norm."]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/plain":["'/home/ashish/image-segmentation'"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["pwd"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-11-27T20:19:11.995465Z","iopub.status.busy":"2022-11-27T20:19:11.995015Z","iopub.status.idle":"2022-11-27T20:21:47.392814Z","shell.execute_reply":"2022-11-27T20:21:47.391016Z","shell.execute_reply.started":"2022-11-27T20:19:11.995428Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Building Training Dataset:   0%|          | 0/18 [00:00<?, ?it/s]\n"]},{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/home/ashish/image-segmentation/content/gtFine/trainweimar'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[10], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m val_masks_enc \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train_file \u001b[38;5;129;01min\u001b[39;00m tqdm(os\u001b[38;5;241m.\u001b[39mlistdir(train_filepath), desc \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBuilding Training Dataset: \u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m---> 13\u001b[0m     image, mask \u001b[38;5;241m=\u001b[39m \u001b[43mimage_mask_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_filepath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIMAGE_SIZE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     train_images\u001b[38;5;241m.\u001b[39mappend(image)\n\u001b[1;32m     15\u001b[0m     train_masks\u001b[38;5;241m.\u001b[39mappend(mask)\n","Cell \u001b[0;32mIn[4], line 6\u001b[0m, in \u001b[0;36mimage_mask_split\u001b[0;34m(filename, image_size)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimage_mask_split\u001b[39m(filename, image_size):\n\u001b[0;32m----> 6\u001b[0m     image_mask \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     image, mask \u001b[38;5;241m=\u001b[39m image_mask\u001b[38;5;241m.\u001b[39mcrop([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m]), image_mask\u001b[38;5;241m.\u001b[39mcrop([\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m256\u001b[39m])\n\u001b[1;32m      9\u001b[0m     image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mresize(image_size)\n","File \u001b[0;32m~/image-segmentation/venv/lib/python3.11/site-packages/PIL/Image.py:3277\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3274\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrealpath(os\u001b[38;5;241m.\u001b[39mfspath(fp))\n\u001b[1;32m   3276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[0;32m-> 3277\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3278\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   3280\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/ashish/image-segmentation/content/gtFine/trainweimar'"]}],"source":["train_filepath = '/home/ashish/image-segmentation/content/gtFine/train'\n","val_filepath = '/home/ashish/image-segmentation/content/gtFine/val'\n","\n","# Store the images, the masks, and the encoded masks\n","train_images = [] \n","train_masks = []\n","train_masks_enc = []\n","val_images = []\n","val_masks = []\n","val_masks_enc = []\n","\n","for train_file in tqdm(os.listdir(train_filepath), desc = 'Building Training Dataset: '):\n","    image, mask = image_mask_split(train_filepath + train_file, IMAGE_SIZE)\n","    train_images.append(image)\n","    train_masks.append(mask)\n","    train_masks_enc.append(find_closest_labels_vectorized(mask, id2color))\n","    \n","for val_file in tqdm(os.listdir(val_filepath), desc = 'Building Validation Dataset: '):\n","    image, mask = image_mask_split(val_filepath + val_file, IMAGE_SIZE)\n","    val_images.append(image)\n","    val_masks.append(mask)\n","    val_masks_enc.append(find_closest_labels_vectorized(mask, id2color))"]},{"cell_type":"markdown","metadata":{},"source":["## Visualize the image, mask, and encoded mask\n","\n","Lets take a look at a couple of the images and how well the encoding did.\n","\n","As stated before, there is an issue with boundary edges and this is quite evident in the encoding. However, it still looks to have done a good job overall with the vast majority of each image being encoded correctly."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-27T20:21:47.394613Z","iopub.status.busy":"2022-11-27T20:21:47.394208Z","iopub.status.idle":"2022-11-27T20:21:48.631585Z","shell.execute_reply":"2022-11-27T20:21:48.624968Z","shell.execute_reply.started":"2022-11-27T20:21:47.394572Z"},"trusted":true},"outputs":[],"source":["plt.figure(figsize=[20, 14])\n","\n","for i in range(2):\n","    img = train_images[i]\n","    msk = train_masks[i]\n","    enc = train_masks_enc[i]\n","    tmp = np.zeros([enc.shape[0], enc.shape[1], 3])\n","    \n","    for row in range(enc.shape[0]):\n","        for col in range(enc.shape[1]):\n","            tmp[row, col, :] = id2color[enc[row, col]]\n","            tmp = tmp.astype('uint8')\n","            \n","    plt.subplot(2, 3, i*3 + 1)\n","    plt.imshow(img)\n","    plt.axis('off')\n","    plt.gca().set_title('Sample Image {}'.format(str(i+1)))\n","    \n","    plt.subplot(2, 3, i*3 + 2)\n","    plt.imshow(msk)\n","    plt.axis('off')\n","    plt.gca().set_title('Sample Mask {}'.format(str(i+1)))\n","    \n","    plt.subplot(2, 3, i*3 + 3)\n","    plt.imshow(tmp)\n","    plt.axis('off')\n","    plt.gca().set_title('Sample Encoded Mask {}'.format(str(i+1)))\n","    \n","plt.subplots_adjust(wspace=0, hspace=0.1)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-27T20:55:01.195282Z","iopub.status.busy":"2022-11-27T20:55:01.191588Z","iopub.status.idle":"2022-11-27T20:55:01.207065Z","shell.execute_reply":"2022-11-27T20:55:01.205939Z","shell.execute_reply.started":"2022-11-27T20:55:01.195216Z"},"trusted":true},"outputs":[],"source":["# delete the masks as they are no longer needed to free up RAM\n","del train_masks, val_masks"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-27T20:21:48.633902Z","iopub.status.busy":"2022-11-27T20:21:48.633374Z","iopub.status.idle":"2022-11-27T20:21:52.300841Z","shell.execute_reply":"2022-11-27T20:21:52.299775Z","shell.execute_reply.started":"2022-11-27T20:21:48.633837Z"},"trusted":true},"outputs":[],"source":["train_images = np.stack(train_images).astype('float32')\n","train_masks_enc = np.stack(train_masks_enc).astype('float32')\n","\n","val_images = np.stack(val_images).astype('float32')\n","val_masks_enc = np.stack(val_masks_enc).astype('float32')"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2022-11-26T20:03:06.471302Z","iopub.status.busy":"2022-11-26T20:03:06.47008Z","iopub.status.idle":"2022-11-26T20:03:06.475379Z","shell.execute_reply":"2022-11-26T20:03:06.474586Z","shell.execute_reply.started":"2022-11-26T20:03:06.471259Z"}},"source":["# Building the U-Net Model\n","\n","This functions to build this U-Net model are highly inspired by Andrew Ng's Deep Learning Specialization\n","\n","There is a defined function for the downscaling convolution block and a function for the upsampling block"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-27T20:21:52.302694Z","iopub.status.busy":"2022-11-27T20:21:52.302315Z","iopub.status.idle":"2022-11-27T20:21:52.313352Z","shell.execute_reply":"2022-11-27T20:21:52.312078Z","shell.execute_reply.started":"2022-11-27T20:21:52.302655Z"},"trusted":true},"outputs":[],"source":["def conv_block(inputs=None, n_filters=32, kernel_size = 3, dropout_prob = 0, max_pooling=True):\n","    \n","    conv = Conv2D(n_filters, # Number of filters\n","                  kernel_size = 3, # Kernel size   \n","                  activation = 'relu',\n","                  padding = 'same',\n","                  kernel_initializer = 'he_normal')(inputs)\n","    \n","    conv = Conv2D(n_filters, # Number of filters\n","                  kernel_size = 3,   # Kernel size\n","                  activation = 'relu',\n","                  padding = 'same',\n","                  kernel_initializer = 'he_normal')(conv)\n","    \n","    # if dropout_prob > 0 add a dropout layer, with the variable dropout_prob as parameter\n","    if dropout_prob > 0:\n","        conv = Dropout(dropout_prob)(conv)\n","        \n","    # if max_pooling is True add a MaxPooling2D with 2x2 pool_size\n","    if max_pooling:\n","        next_layer = MaxPooling2D(pool_size = (2,2))(conv)\n","    else:\n","        next_layer = conv\n","        \n","    skip_connection = conv\n","    \n","    return next_layer, skip_connection\n","\n","def upsampling_block(expansive_input, contractive_input, n_filters=32, kernel_size = 3):\n","    \n","    up = Conv2DTranspose(\n","                 n_filters,    # number of filters\n","                 kernel_size = kernel_size,    # Kernel size\n","                 strides = (2,2),\n","                 padding = 'same')(expansive_input)\n","    \n","    # Merge the previous output and the contractive_input\n","    merge = concatenate([up, contractive_input], axis=3)\n","    \n","    conv = Conv2D(n_filters,   # Number of filters\n","                 kernel_size = (3,3),     # Kernel size\n","                 activation='relu',\n","                 padding='same',\n","                 kernel_initializer='he_normal')(merge)\n","    conv = Conv2D(n_filters,  # Number of filters\n","                 kernel_size = (3,3),   # Kernel size\n","                 activation='relu',\n","                 padding='same',\n","                 kernel_initializer='he_normal')(conv)\n","    \n","    return conv"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-27T20:21:52.316025Z","iopub.status.busy":"2022-11-27T20:21:52.3148Z","iopub.status.idle":"2022-11-27T20:21:52.329611Z","shell.execute_reply":"2022-11-27T20:21:52.328556Z","shell.execute_reply.started":"2022-11-27T20:21:52.315974Z"},"trusted":true},"outputs":[],"source":["def create_unet_model(image_shape, n_filters, kernel_size, n_classes):\n","\n","    inputs = Input(image_shape)\n","\n","    # Contracting Path (encoding)\n","    cblock1 = conv_block(inputs, n_filters, kernel_size)\n","    cblock2 = conv_block(cblock1[0], n_filters * 2, kernel_size)\n","    cblock3 = conv_block(cblock2[0], n_filters * 4, kernel_size, dropout_prob = 0.3)\n","    cblock4 = conv_block(cblock3[0], n_filters * 8, kernel_size, dropout_prob = 0.3) # Include a dropout_prob of 0.3 for this layer\n","    cblock5 = conv_block(cblock4[0], n_filters * 16, kernel_size, dropout_prob = 0.3, max_pooling=False) \n","\n","    # Expanding Path (decoding)\n","    # Add the first upsampling_block.\n","\n","    ublock6 = upsampling_block(cblock5[0], cblock4[1], n_filters * 8, kernel_size)\n","    ublock7 = upsampling_block(ublock6, cblock3[1], n_filters * 4, kernel_size)\n","    ublock8 = upsampling_block(ublock7, cblock2[1], n_filters * 2, kernel_size)\n","    ublock9 = upsampling_block(ublock8, cblock1[1], n_filters, kernel_size)\n","\n","    conv9 = Conv2D(n_filters,\n","                 kernel_size = kernel_size,\n","                 activation='relu',\n","                 padding='same',\n","                 kernel_initializer='he_normal')(ublock9)\n","\n","    # Add a Conv2D layer with n_classes filter, kernel size of 1 and a 'same' padding\n","    conv10 = Conv2D(n_classes, kernel_size = 1, padding='same')(conv9)\n","\n","    model = tf.keras.Model(inputs=inputs, outputs=conv10)\n","    \n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-27T20:25:49.927842Z","iopub.status.busy":"2022-11-27T20:25:49.92685Z","iopub.status.idle":"2022-11-27T20:25:50.825513Z","shell.execute_reply":"2022-11-27T20:25:50.820961Z","shell.execute_reply.started":"2022-11-27T20:25:49.92779Z"},"trusted":true},"outputs":[],"source":["# use the functions to build the model and display it below\n","model = create_unet_model(IMAGE_SHAPE, N_FILTERS, KERNEL_SIZE, N_CLASSES)\n","\n","tf.keras.utils.plot_model(model, show_shapes = True)"]},{"cell_type":"markdown","metadata":{},"source":["# Model Training\n","\n","The model checkpoint callback will be used to save only the best epoch.\n","\n","There is no early stopping as I want to visually see whether the model begins to overfit and to what extent."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-27T20:25:51.676635Z","iopub.status.busy":"2022-11-27T20:25:51.676131Z","iopub.status.idle":"2022-11-27T20:43:17.055941Z","shell.execute_reply":"2022-11-27T20:43:17.054728Z","shell.execute_reply.started":"2022-11-27T20:25:51.676592Z"},"trusted":true},"outputs":[],"source":["model.compile(optimizer = 'adam',\n","              loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","              metrics = ['accuracy'])\n","\n","model_checkpoint = ModelCheckpoint(MODEL_CHECKPOINT_FILEPATH,\n","                                   monitor='val_accuracy',\n","                                   save_best_only=True,\n","                                   save_weights_only=True,\n","                                   verbose=1,\n","                                   mode = 'max')\n","\n","callbacks = [model_checkpoint]\n","\n","history = model.fit(x = train_images,\n","                    y = train_masks_enc,\n","                    batch_size = BATCH_SIZE,\n","                    epochs = EPOCHS,\n","                    validation_data = (val_images, val_masks_enc),\n","                    callbacks = callbacks)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-27T20:43:26.194748Z","iopub.status.busy":"2022-11-27T20:43:26.194251Z","iopub.status.idle":"2022-11-27T20:43:27.510115Z","shell.execute_reply":"2022-11-27T20:43:27.508897Z","shell.execute_reply.started":"2022-11-27T20:43:26.194708Z"},"trusted":true},"outputs":[],"source":["fig, (ax1, ax2) = plt.subplots(1,2, figsize=(16,6))\n","title_fontsize = 16\n","axis_fontsize = 12\n","\n","ax1.plot(range(1, EPOCHS + 1), history.history['loss'], marker='o', label='Training loss')\n","ax1.plot(range(1, EPOCHS + 1), history.history['val_loss'], marker='o', label='Validation Loss')\n","ax1.legend()\n","ax1.set_xticks(range(1, EPOCHS + 1))\n","ax1.set_title('Loss', fontsize=title_fontsize)\n","ax1.set_xlabel('Epoch', fontsize=axis_fontsize)\n","\n","ax2.plot(range(1, EPOCHS + 1), history.history['accuracy'], marker='o', label='Training Accuracy')\n","ax2.plot(range(1, EPOCHS + 1), history.history['val_accuracy'], marker='o', label='Validation Accuracy')\n","ax2.legend()\n","ax2.set_xticks(range(1, EPOCHS + 1))\n","ax2.set_title('Accuracy', fontsize=title_fontsize)\n","ax2.set_xlabel('Epoch', fontsize=axis_fontsize);"]},{"cell_type":"markdown","metadata":{},"source":["# Model Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-27T20:43:32.90159Z","iopub.status.busy":"2022-11-27T20:43:32.901194Z","iopub.status.idle":"2022-11-27T20:43:43.788739Z","shell.execute_reply":"2022-11-27T20:43:43.787534Z","shell.execute_reply.started":"2022-11-27T20:43:32.901559Z"},"trusted":true},"outputs":[],"source":["model.load_weights(MODEL_CHECKPOINT_FILEPATH) # load the best model weights\n","\n","val_loss, val_accuracy = model.evaluate(x = val_images, y = val_masks_enc) # re-evaluate on the validation data\n","\n","print('\\n\\033[1m' + 'The model had an accuracy score of {}%!!'.format(round(100*val_accuracy, 2)) + '\\033[0m')"]},{"cell_type":"markdown","metadata":{},"source":["### Let's see how the model does on some on the validation images.\n","\n","We will visually look side-by-side as the true model mask and the predicted model mask on some of the images from the validation set"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-27T20:43:45.421432Z","iopub.status.busy":"2022-11-27T20:43:45.420613Z","iopub.status.idle":"2022-11-27T20:43:49.092423Z","shell.execute_reply":"2022-11-27T20:43:49.090106Z","shell.execute_reply.started":"2022-11-27T20:43:45.421386Z"},"trusted":true},"outputs":[],"source":["plt.figure(figsize=[15, 20])\n","\n","for i in range(4):    \n","    img = val_images[i]\n","    enc = val_masks_enc[i]\n","    \n","    pred = model.predict(img.reshape([1] + IMAGE_SHAPE))\n","    pred = np.squeeze(np.argmax(pred, axis=-1))\n","    \n","    tmp1 = np.zeros([enc.shape[0], enc.shape[1], 3])\n","    tmp2 = np.zeros([enc.shape[0], enc.shape[1], 3])\n","    \n","    \n","    for row in range(enc.shape[0]):\n","        for col in range(enc.shape[1]):\n","            tmp1[row, col, :] = id2color[enc[row, col]]\n","            tmp1 = tmp1.astype('uint8')\n","                     \n","            tmp2[row, col, :] = id2color[pred[row, col]]\n","            tmp2 = tmp2.astype('uint8')\n","            \n","    plt.subplot(4, 3, i*3 + 1)\n","    plt.imshow(img)\n","    plt.axis('off')\n","    plt.gca().set_title('Image {}'.format(str(i+1)))\n","    \n","    plt.subplot(4, 3, i*3 + 2)\n","    plt.imshow(tmp1)\n","    plt.axis('off')\n","    plt.gca().set_title('Encoded Mask {}'.format(str(i+1)))\n","    \n","    plt.subplot(4, 3, i*3 + 3)\n","    plt.imshow(tmp2)\n","    plt.axis('off')\n","    plt.gca().set_title('Model Prediction {}'.format(str(i+1)))\n","    \n","plt.subplots_adjust(wspace=0, hspace=0.1)"]},{"cell_type":"markdown","metadata":{},"source":["### If you liked this notebook, please leave an upvote!"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":22655,"sourceId":29047,"sourceType":"datasetVersion"}],"dockerImageVersionId":30302,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":4}
